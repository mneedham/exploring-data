# Exploring UK petitions

Setup the environment and load in appropriate libraries

```{r  message=FALSE}
library(dplyr)
```

Load the data and tidy up columns

```{r message=FALSE}
df = read.csv("../data/all_the_petitions.csv", na.strings = c("", "NA"))

cleanDate = function(dateString) {
  strptime(dateString, "%Y-%m-%dT%H:%M:%S") %>% as.POSIXct
}

df$createdAt = cleanDate(df$createdAt)
df$updatedAt = cleanDate(df$updatedAt)
df$openAt = cleanDate(df$openAt)
df$closedAt = cleanDate(df$closedAt)
df$governmentResponseAt = cleanDate(df$governmentResponseAt)
df$scheduledDebateDate = cleanDate(df$scheduledDebateDate)
df$responseThresholdReachedAt = cleanDate(df$responseThresholdReachedAt)

df %>% select(action, createdAt, state, signatureCount) %>% head()
```

## How many are not closed yet?

```{r message=FALSE}
df %>% filter(!is.na(closedAt)) %>% count()
```

## How many petitions got a government response?

```{r message=FALSE}
governmentResponse = df %>% filter(!is.na(governmentResponseAt))
governmentResponse %>% count()
```

## What were they?

```{r message=FALSE}
governmentResponse %>% select(link, action, createdAt, state, signatureCount) %>% head()
```

## What have been the most popular petitions ever?

```{r message=FALSE}
df %>% arrange(desc(signatureCount)) %>% select(link, action, createdAt, state, signatureCount) %>% head()
```

Time for a bit of text processing based on this article - https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html#explore-your-data

```{r message=FALSE}
 Needed <- c("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc") 
# Uncomment these lines to install the packages
 
#install.packages(Needed, dependencies=TRUE)   
   
# install.packages("Rcampdf", repos = "http://datacube.wu.ac.at/", type = "source")
```
 
# Load and clean the text 
 
```{r message=FALSE} 
library(tm)
library(SnowballC)   

ds = DataframeSource(df %>% select(action))
docs = VCorpus(ds)
docs <- tm_map(docs, removePunctuation)   
docs <- tm_map(docs, removeNumbers) 
docs <- tm_map(docs, tolower)   
docs <- tm_map(docs, removeWords, stopwords("english"))  
docs <- tm_map(docs, stemDocument)   
docs <- tm_map(docs, stripWhitespace)  
docs <- tm_map(docs, PlainTextDocument)  
```

 
```{r message=FALSE}
dtm = DocumentTermMatrix(docs)   
tdm = TermDocumentMatrix(docs)   
 
dtms <- removeSparseTerms(dtm, 0.1) # This makes a matrix that is 10% empty space, maximum.   

freq <- colSums(as.matrix(dtm))   
ord <- order(freq)   
```
 
## Most commonly used words in petitions
 
```{r message = FALSE}
freq[tail(ord)]   
```


```{r message = FALSE}
library(wordcloud)

set.seed(142)   
wordcloud(names(freq), freq, min.freq=25)   

set.seed(142)   
wordcloud(names(freq), freq, max.words=100)  

set.seed(142)   
wordcloud(names(freq), freq, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))   
```

## Find frequent terms in petitions

```{r message=FALSE}
findFreqTerms(dtm, lowfreq=50) 

wf <- data.frame(word=names(freq), freq=freq)   
wf %>% arrange(desc(freq)) %>% head()
```

## Plot the most popular words

```{r message=FALSE}
library(ggplot2)   
p <- ggplot(subset(wf, freq>50), aes(word, freq))    
p <- p + geom_bar(stat="identity")   
p <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   
p   
```

## Word correlations

```{r message=FALSE}
findAssocs(dtm, c("tax", "google"), corlimit=0.1)
```

## How many petitions are about tax?

```{r message=FALSE}
df %>% filter(grepl("tax", action)) %>% count()
```

## And how many about evasion?

```{r message=FALSE}
df %>% filter(grepl("evas", action)) %>% count()
```

## The underlying word stemming function

```{r message=FALSE}
wordStem(c("evasion"))
```

## Successful petitions

You have to get 10,000 signatures for a government response
You have to get 100,000 signatures for a debate in parliament

```{r message=FALSE}
df %>% filter(signatureCount > 10000) %>% count()
df %>% filter(signatureCount > 100000) %>% count()
```

According to https://petition.parliament.uk/:

172 petitions got a response from the Government
22 petitions were debated in the House of Commons

which means some petitions which fitted into those categories haven't been responded to yet for some reason.

```{r message=FALSE}
df %>% filter(signatureCount > 10000 & is.na(governmentResponseAt)) %>% select(link, action, signatureCount)
```

Let's split ones which get less than 10,000 signatures and ones that get more than 10,000 signatures and see if the words used are any different.

```{r message=FALSE}
success = df %>% filter(signatureCount > 10000)
not_success = df %>% filter(signatureCount < 10000)
```

```{r message=FALSE}
createCorpus = function(df) {
  ds = DataframeSource(df %>% select(action))
  docs = VCorpus(ds)
  docs <- tm_map(docs, removePunctuation)   
  docs <- tm_map(docs, removeNumbers) 
  docs <- tm_map(docs, tolower)   
  docs <- tm_map(docs, removeWords, stopwords("english"))  
  docs <- tm_map(docs, stemDocument)   
  docs <- tm_map(docs, stripWhitespace)  
  docs <- tm_map(docs, PlainTextDocument) 
  return(docs)
}
```

```{r message=FALSE}
successCorpus = createCorpus(success %>% select(action))
success_dtm = DocumentTermMatrix(successCorpus)   
success_tdm = TermDocumentMatrix(successCorpus)   
 
success_dtms <- removeSparseTerms(success_dtm, 0.1) # This makes a matrix that is 10% empty space, maximum. 
success_freq <- colSums(as.matrix(success_dtm))   
success_ord <- order(success_freq) 

data.frame(word=names(success_freq), freq=success_freq) %>% arrange(desc(freq)) %>% head()
```

```{r message=FALSE}
notSuccessCorpus = createCorpus(not_success %>% select(action))
notSuccess_dtm = DocumentTermMatrix(notSuccessCorpus)   
notSuccess_tdm = TermDocumentMatrix(notSuccessCorpus)   
 
notSuccess_dtms <- removeSparseTerms(success_dtm, 0.1) # This makes a matrix that is 10% empty space, maximum. 
notSuccess_freq <- colSums(as.matrix(notSuccess_dtm))   
notSuccess_ord <- order(notSuccess_freq) 

data.frame(word=names(notSuccess_freq), freq=notSuccess_freq) %>% arrange(desc(freq)) %>% head()
```

It seems like the same type of words show up both in successful petitions and in unsucessful ones. 
It's all about stopping, making or banning something!

At the moment we have the frequency of these words across the whole set of documents but we don't have the frequency per 1000 words like in the Whitehouse version - http://www.prooffreader.com/2016/03/most-characteristic-words-in-successful.html?m=1

We also haven't yet looked into the full text of a petition, only the titles.
We also haven't looked at phrases (n-grams) that may occur in the documents - need to figure out how to do that with the tm package.
